{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Lab\n",
    "![LeNet Architecture](lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the MNIST data, which comes pre-loaded with TensorFlow.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Image Shape: (28, 28, 1)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ae86d2bbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    # Filter (weights and bias)\n",
    "    #Need stride = 1, filter_height = 5, to get output 28 from 32:\n",
    "    #out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "    #28 = ceil((32-5 + 1) /1)\n",
    "    #out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2])) (same as above)\n",
    "    F_W = tf.Variable(tf.truncated_normal((5, 5, 1, 6),mu,sigma)) # (height, width, input_depth, output_depth)\n",
    "    F_b = tf.Variable(tf.zeros(6)) # (output_depth)\n",
    "    strides = [1, 1, 1, 1]\n",
    "    padding = 'VALID'\n",
    "    #print(\"F_W1:\",F_W)\n",
    "    #print(\"F_b1:\",F_b)\n",
    "    conv_layer = tf.nn.conv2d(x, F_W, strides, padding)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, F_b)\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    #out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "    #Selecting ksize and strides as 2 in both dims to get output size of 14:\n",
    "    #14 = ceil(28 - 2 + 1) / 2\n",
    "    #out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "    \n",
    "    #print(conv_layer)\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    # Filter (weights and bias)\n",
    "    #Need stride = 1, filter_height = 5, to get output 10 from 14:\n",
    "    #out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "    #10 = ceil((14-5 + 1) /1)\n",
    "    #out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2])) (same as above)\n",
    "    F_W = tf.Variable(tf.truncated_normal((5, 5, 6, 16),mu,sigma)) # (height, width, input_depth, output_depth)\n",
    "    F_b = tf.Variable(tf.zeros(16)) # (output_depth)\n",
    "    strides = [1, 1, 1, 1]\n",
    "    padding = 'VALID'\n",
    "    #print(\"F_W2:\",F_W)\n",
    "    #print(\"F_b2:\",F_b)\n",
    "    conv_layer = tf.nn.conv2d(conv_layer, F_W, strides, padding)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, F_b)\n",
    "    \n",
    "    #print(conv_layer)\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    #out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "    #Selecting ksize and strides as 2 in both dims to get output size of 5:\n",
    "    #5 = ceil(10 - 2 + 1) / 2\n",
    "    #out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "    \n",
    "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
    "    conv_layer = tf.contrib.layers.flatten(conv_layer)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.    \n",
    "    n_input = 400\n",
    "    n_hidden_layer = 120\n",
    "    \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    conv_layer = tf.add(tf.matmul(conv_layer, weights['hidden_layer']), biases['hidden_layer'])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    n_input = 120\n",
    "    n_hidden_layer = 84\n",
    "    \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    conv_layer = tf.add(tf.matmul(conv_layer, weights['hidden_layer']), biases['hidden_layer'])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    n_input = 84\n",
    "    n_hidden_layer = 10\n",
    "    \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    logits = tf.add(tf.matmul(conv_layer, weights['hidden_layer']), biases['hidden_layer'])\n",
    "    print(logits)\n",
    "    return logits\n",
    "\n",
    "#x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "#LeNet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)\n",
    "#print(one_hot_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.817\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.881\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.930\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.960\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.962\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.961\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is: <class 'numpy.ndarray'> with dimesions: (32, 32, 4)\n",
      "[[[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aeac08f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "#import pandas as pd\n",
    "#from pandas import *\n",
    "%matplotlib inline\n",
    "\n",
    "#reading in an image\n",
    "image = mpimg.imread('mytestimg.bmp')\n",
    "#printing out some stats and plotting\n",
    "print('This image is:', type(image), 'with dimesions:', image.shape)\n",
    "plt.imshow(image)  # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')\n",
    "\n",
    "#normalize\n",
    "image = image.astype('float32')/255\n",
    "#pd.set_option('display.max_columns', 32)\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
